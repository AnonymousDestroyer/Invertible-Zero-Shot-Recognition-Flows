{"format": "torch", "nodes": [{"name": "transforms.0", "id": 139849359826768, "class_name": "AffineCoupling(\n  (nn_s): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n  (nn_t): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n)", "parameters": [["nn_s.in_layer.weight", [2, 2]], ["nn_s.in_layer.bias", [2]], ["nn_s.out_layer.weight", [2, 2]], ["nn_s.out_layer.bias", [2]], ["nn_s.layers.0.weight", [2, 2]], ["nn_s.layers.0.bias", [2]], ["nn_t.in_layer.weight", [2, 2]], ["nn_t.in_layer.bias", [2]], ["nn_t.out_layer.weight", [2, 2]], ["nn_t.out_layer.bias", [2]], ["nn_t.layers.0.weight", [2, 2]], ["nn_t.layers.0.bias", [2]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2]}, {"name": "transforms.1", "id": 139849359823184, "class_name": "ActNormBijection()", "parameters": [["shift", [1, 4]], ["log_scale", [1, 4]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 4]}, {"name": "transforms.2", "id": 139849359826000, "class_name": "LinearLU()", "parameters": [["lower_entries", [6]], ["upper_entries", [6]], ["unconstrained_upper_diag", [4]]], "output_shape": [[256, 4], [256]], "num_parameters": [6, 6, 4]}, {"name": "transforms.3", "id": 139849359824528, "class_name": "AffineCoupling(\n  (nn_s): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n  (nn_t): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n)", "parameters": [["nn_s.in_layer.weight", [2, 2]], ["nn_s.in_layer.bias", [2]], ["nn_s.out_layer.weight", [2, 2]], ["nn_s.out_layer.bias", [2]], ["nn_s.layers.0.weight", [2, 2]], ["nn_s.layers.0.bias", [2]], ["nn_t.in_layer.weight", [2, 2]], ["nn_t.in_layer.bias", [2]], ["nn_t.out_layer.weight", [2, 2]], ["nn_t.out_layer.bias", [2]], ["nn_t.layers.0.weight", [2, 2]], ["nn_t.layers.0.bias", [2]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2]}, {"name": "transforms.4", "id": 139849359823632, "class_name": "ActNormBijection()", "parameters": [["shift", [1, 4]], ["log_scale", [1, 4]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 4]}, {"name": "transforms.5", "id": 139849359880272, "class_name": "LinearLU()", "parameters": [["lower_entries", [6]], ["upper_entries", [6]], ["unconstrained_upper_diag", [4]]], "output_shape": [[256, 4], [256]], "num_parameters": [6, 6, 4]}, {"name": "transforms.6", "id": 139849359880656, "class_name": "AffineCoupling(\n  (nn_s): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n  (nn_t): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n)", "parameters": [["nn_s.in_layer.weight", [2, 2]], ["nn_s.in_layer.bias", [2]], ["nn_s.out_layer.weight", [2, 2]], ["nn_s.out_layer.bias", [2]], ["nn_s.layers.0.weight", [2, 2]], ["nn_s.layers.0.bias", [2]], ["nn_t.in_layer.weight", [2, 2]], ["nn_t.in_layer.bias", [2]], ["nn_t.out_layer.weight", [2, 2]], ["nn_t.out_layer.bias", [2]], ["nn_t.layers.0.weight", [2, 2]], ["nn_t.layers.0.bias", [2]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2]}, {"name": "transforms.7", "id": 139849359946576, "class_name": "ActNormBijection()", "parameters": [["shift", [1, 4]], ["log_scale", [1, 4]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 4]}, {"name": "transforms.8", "id": 139849359947408, "class_name": "LinearLU()", "parameters": [["lower_entries", [6]], ["upper_entries", [6]], ["unconstrained_upper_diag", [4]]], "output_shape": [[256, 4], [256]], "num_parameters": [6, 6, 4]}, {"name": "transforms.9", "id": 139849359947472, "class_name": "AffineCoupling(\n  (nn_s): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n  (nn_t): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n)", "parameters": [["nn_s.in_layer.weight", [2, 2]], ["nn_s.in_layer.bias", [2]], ["nn_s.out_layer.weight", [2, 2]], ["nn_s.out_layer.bias", [2]], ["nn_s.layers.0.weight", [2, 2]], ["nn_s.layers.0.bias", [2]], ["nn_t.in_layer.weight", [2, 2]], ["nn_t.in_layer.bias", [2]], ["nn_t.out_layer.weight", [2, 2]], ["nn_t.out_layer.bias", [2]], ["nn_t.layers.0.weight", [2, 2]], ["nn_t.layers.0.bias", [2]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2]}, {"name": "transforms.10", "id": 139849359948432, "class_name": "ActNormBijection()", "parameters": [["shift", [1, 4]], ["log_scale", [1, 4]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 4]}, {"name": "transforms.11", "id": 139849359949264, "class_name": "LinearLU()", "parameters": [["lower_entries", [6]], ["upper_entries", [6]], ["unconstrained_upper_diag", [4]]], "output_shape": [[256, 4], [256]], "num_parameters": [6, 6, 4]}, {"name": "transforms.12", "id": 139849359949456, "class_name": "AffineCoupling(\n  (nn_s): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n  (nn_t): MLP(\n    (non_linearity): LeakyReLU(negative_slope=0.01)\n    (in_layer): Linear(in_features=2, out_features=2, bias=True)\n    (out_layer): Linear(in_features=2, out_features=2, bias=True)\n    (layers): ModuleList(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n    )\n  )\n)", "parameters": [["nn_s.in_layer.weight", [2, 2]], ["nn_s.in_layer.bias", [2]], ["nn_s.out_layer.weight", [2, 2]], ["nn_s.out_layer.bias", [2]], ["nn_s.layers.0.weight", [2, 2]], ["nn_s.layers.0.bias", [2]], ["nn_t.in_layer.weight", [2, 2]], ["nn_t.in_layer.bias", [2]], ["nn_t.out_layer.weight", [2, 2]], ["nn_t.out_layer.bias", [2]], ["nn_t.layers.0.weight", [2, 2]], ["nn_t.layers.0.bias", [2]]], "output_shape": [[256, 4], [256]], "num_parameters": [4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2]}], "edges": []}